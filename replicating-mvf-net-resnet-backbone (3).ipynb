{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10977544,"sourceType":"datasetVersion","datasetId":6831064},{"sourceId":13961703,"sourceType":"datasetVersion","datasetId":8896393}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, csv, time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy.io as sio\nfrom collections import OrderedDict\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PATH\nSELECTED_FOLDERS = [\"LFPW\"] # tạm thời chỉ lấy 1 để tiết kiệm thời gian\nDATA_ROOT = \"/kaggle/input/300w-lp/300W_LP\"\nTRIPLETS_CSV = \"/kaggle/input/multi-view-input/triplets_shorter_LFPW.csv\"\nWEIGHTS_PATH = \"/kaggle/input/multi-view-input/resnet50-11ad3fa6.pth\"\nBATCH_SIZE = 16 # lớn hơn Kaggle không chạy được\nNUM_EPOCHS = 2\nLR = 1e-5\nCHECKPOINT_PATH = \"resnet50_mvfnet.pth\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_params_from_mat(mat_path):\n    \"\"\"\n    Return params vector (concatenate of Shape_Para, Exp_Para, Pose_Para)\n    \"\"\"\n    mat = sio.loadmat(mat_path)\n    \n    shape = np.array(mat.get(\"Shape_Para\", np.zeros((1, 0)))).reshape(-1)\n    exp = np.array(mat.get(\"Exp_Para\", np.zeros((1, 0)))).reshape(-1)\n    pose = np.array(mat.get(\"Pose_Para\", np.zeros((1, 0)))).reshape(-1)\n\n    params = np.concatenate([shape, exp, pose]).astype(np.float32)\n    return params","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_folder_from_filename(filename):\n    for folder in SELECTED_FOLDERS:\n        if filename.startswith(folder):\n            return folder\n    raise ValueError(f\"Cannot find folder for {filename}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Triplet300wLP(Dataset):\n    def __init__(self, triplets_csv, dataset_path, transform=None):\n        self.data = pd.read_csv(triplets_csv)\n        self.dataset_path = dataset_path\n        self.transform = transform\n        \n        # Normalization constants (from your statistics)\n        self.shape_std = 66912.0   # From your data check\n        self.exp_std = 0.6         # From your data check\n        self.pose_std = 107.9      # From your data check\n        \n        # Create folder mapping\n        self.folder_map = {}\n        for folder in SELECTED_FOLDERS:\n            folder_path = os.path.join(dataset_path, folder)\n            if not os.path.exists(folder_path):\n                continue\n            for f in os.listdir(folder_path):\n                if f.endswith('.jpg') or f.endswith('.mat'):\n                    self.folder_map[f] = folder\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        imgs = []\n        front_label = None\n        \n        for view in [\"front\", \"left\", \"right\"]:\n            filename_img = row[f'{view}_img']\n            filename_mat = row[f'{view}_mat']\n            \n            folder = get_folder_from_filename(filename_img)\n            img_path = os.path.join(self.dataset_path, folder, filename_img)\n            mat_path = os.path.join(self.dataset_path, folder, filename_mat)\n            \n            img = Image.open(img_path).convert(\"RGB\")\n            if self.transform:\n                img = self.transform(img)\n            else:\n                img = transforms.ToTensor()(img)\n            imgs.append(img)\n            \n            if view == \"front\":\n                mat_data = sio.loadmat(mat_path)\n                shape_para = mat_data[\"Shape_Para\"].flatten()\n                exp_para = mat_data[\"Exp_Para\"].flatten()\n                pose_para = mat_data[\"Pose_Para\"].flatten()\n                \n                # NORMALIZE\n                shape_para = shape_para / self.shape_std\n                exp_para = exp_para / self.exp_std\n                pose_para = pose_para / self.pose_std\n                \n                front_label = np.concatenate([shape_para, exp_para, pose_para], axis=0)\n        \n        input_tensor = torch.cat(imgs, dim=0)\n        label_tensor = torch.from_numpy(front_label).float()\n        return input_tensor, label_tensor","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\n# Replace dataset creation\ndataset = Triplet300wLP(TRIPLETS_CSV, DATA_ROOT, transform=transform)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNetEncoder(nn.Module):\n    def __init__(self, feat_dim=512, num_shape=199, num_exp=29, num_pose=7, weights_path=None):\n        super().__init__()\n\n        self.num_shape = num_shape\n        self.num_exp = num_exp\n        self.num_pose = num_pose\n        \n        # Load ResNet50\n        if weights_path is not None:\n            # Tạo model và load state_dict từ file local\n            base_model = models.resnet50(weights=None)\n            state_dict = torch.load(weights_path)\n            base_model.load_state_dict(state_dict)\n        else:\n            # Không dùng pretrained\n            base_model = models.resnet50(weights=None)\n        \n        # Chỉ giữ convolutional layers\n        self.backbone = nn.Sequential(*list(base_model.children())[:-2])\n        self.avgpool = nn.AdaptiveAvgPool2d(1)  # output: (batch, 2048, 1, 1)\n        self.feat_dim = 2048  # ResNet50 output channel\n\n        # Cải tiến: Learnable fusion weights\n        self.w = nn.Parameter(torch.ones(3, self.feat_dim))\n        \n        # Multi-view fusion for shape and expression\n        self.fc_shape = nn.Sequential(\n            nn.Linear(self.feat_dim * 3, 512),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_shape)\n        )\n        \n        self.fc_exp = nn.Sequential(\n            nn.Linear(self.feat_dim * 3, 512),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_exp)\n        )\n        \n        # View-specific pose prediction\n        self.fc_pose = nn.Sequential(\n            nn.Linear(self.feat_dim, 256),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_pose)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights for newly added layers\"\"\"\n        for m in [self.fc_shape, self.fc_exp, self.fc_pose]:\n            for layer in m.modules():\n                if isinstance(layer, nn.Linear):\n                    nn.init.xavier_normal_(layer.weight)\n                    if layer.bias is not None:\n                        nn.init.constant_(layer.bias, 0)\n    \n    def forward(self, x):\n        \"\"\"\n        x: (B, 9, H, W) -> front|left|right concatenated along channel\n        output: concatenated [3DMM, poseA, poseB, poseC]\n        \"\"\"\n\n        batch_size = x.size(0)\n        \n        # Tách 3 view\n        front = x[:, 0:3, :, :]\n        left = x[:, 3:6, :, :]\n        right = x[:, 6:9, :, :]\n\n        # Trích xuất feature backbone\n        feat_a = self.backbone(front)\n        feat_a = self.avgpool(feat_a).view(batch_size, -1)\n        feat_b = self.backbone(left)\n        feat_b = self.avgpool(feat_b).view(batch_size, -1)\n        feat_c = self.backbone(right)\n        feat_c = self.avgpool(feat_c).view(batch_size, -1)\n\n        # Weighted sum fusion\n        feat_a = feat_a * self.w[0]   # (B, feat_dim)\n        feat_b = feat_b * self.w[1]\n        feat_c = feat_c * self.w[2]\n    \n        # Concatenate weighted features\n        feat_fused = torch.cat([feat_a, feat_b, feat_c], dim=1)  # (B, 6144)\n        \n        # Predict view-invariant parameters\n        shape_params = self.fc_shape(feat_fused)  # (B, 199)\n        exp_params = self.fc_exp(feat_fused)      # (B, 29)\n        \n        # Predict view-specific pose parameters (use original unweighted features)\n        # Re-extract features without weighting for pose prediction\n        feat_a_orig = self.backbone(front)\n        feat_a_orig = self.avgpool(feat_a_orig).view(batch_size, -1)\n        pose_a = self.fc_pose(feat_a_orig)\n        \n        feat_b_orig = self.backbone(left)\n        feat_b_orig = self.avgpool(feat_b_orig).view(batch_size, -1)\n        pose_b = self.fc_pose(feat_b_orig)\n        \n        feat_c_orig = self.backbone(right)\n        feat_c_orig = self.avgpool(feat_c_orig).view(batch_size, -1)\n        pose_c = self.fc_pose(feat_c_orig)\n        \n        # Concatenate all outputs\n        output = torch.cat([shape_params, exp_params, pose_a, pose_b, pose_c], dim=1)\n        \n        return output","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ResNetEncoder(weights_path=WEIGHTS_PATH).to(device)\nmodel","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def supervised_loss(pred, target, lambda_shape=1.0, lambda_exp=1.0, lambda_pose=1.0):\n    \"\"\"\n    Compute supervised loss for MVF-Net on 300W-LP dataset\n    \n    Args:\n        pred: (B, 249) - Model predictions\n              [Shape(199) | Exp(29) | PoseFront(7) | PoseLeft(7) | PoseRight(7)]\n              Indices: [0:199 | 199:228 | 228:235 | 235:242 | 242:249]\n              \n        target: (B, 235) - Ground truth from .mat files\n                [Shape(199) | Exp(29) | PoseFront(7)]\n                Indices: [0:199 | 199:228 | 228:235]\n        \n        lambda_shape: Weight for shape loss\n        lambda_exp: Weight for expression loss  \n        lambda_pose: Weight for pose loss\n    \n    Returns:\n        total_loss: Weighted sum of all losses\n        loss_dict: Dictionary with individual loss values for logging\n    \"\"\"\n    \n    pred_shape = pred[:, :199]        # Shape parameters (0:199)\n    pred_exp = pred[:, 199:228]       # Expression parameters (199:228)\n    pred_pose_front = pred[:, 228:235]  # Front view pose (228:235)\n    \n\n    target_shape = target[:, :199]      # Shape parameters (0:199)\n    target_exp = target[:, 199:228]     # Expression parameters (199:228)\n    target_pose = target[:, 228:235]    # Front view pose (228:235)\n    \n    # Compute losses\n    loss_shape = F.mse_loss(pred_shape, target_shape)\n    loss_exp = F.mse_loss(pred_exp, target_exp)\n    loss_pose = F.mse_loss(pred_pose_front, target_pose)\n    \n    # Weighted total loss\n    total_loss = (lambda_shape * loss_shape + \n                  lambda_exp * loss_exp + \n                  lambda_pose * loss_pose)\n    \n    # Loss dictionary for logging\n    loss_dict = {\n        'total': total_loss.item(),\n        'shape': loss_shape.item(),\n        'exp': loss_exp.item(),\n        'pose': loss_pose.item()\n    }\n    \n    return total_loss, loss_dict","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=LR)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, device, epoch_num):\n    \"\"\"\n    Complete training loop for one epoch\n    \"\"\"\n    model.train()\n    \n    # Track losses\n    total_samples = 0\n    epoch_losses = {\n        'total': 0.0,\n        'shape': 0.0,\n        'exp': 0.0,\n        'pose': 0.0\n    }\n    \n    for batch_idx, (imgs, targets) in enumerate(dataloader):\n        imgs = imgs.to(device)      # (B, 9, H, W)\n        targets = targets.to(device)  # (B, 235)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(imgs)  # (B, 249)\n        \n        # Compute loss\n        loss, loss_dict = supervised_loss(outputs, targets)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # Accumulate losses\n        batch_size = imgs.size(0)\n        total_samples += batch_size\n        for key in epoch_losses:\n            epoch_losses[key] += loss_dict[key] * batch_size\n        \n        # Print progress every 100 batches\n        if (batch_idx + 1) % 100 == 0:\n            print(f\"  Batch {batch_idx+1}/{len(dataloader)} | \"\n                  f\"Loss: {loss_dict['total']:.6f} \"\n                  f\"(shape: {loss_dict['shape']:.6f}, \"\n                  f\"exp: {loss_dict['exp']:.6f}, \"\n                  f\"pose: {loss_dict['pose']:.6f})\")\n    \n    # Average losses over epoch\n    for key in epoch_losses:\n        epoch_losses[key] /= total_samples\n    \n    return epoch_losses","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_data_statistics(dataloader):\n    \"\"\"\n    Check if your data has reasonable values\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"CHECKING DATA STATISTICS\")\n    print(\"=\"*60)\n    \n    # Get one batch\n    imgs, targets = next(iter(dataloader))\n    \n    print(f\"\\nInput Images:\")\n    print(f\"  Shape: {imgs.shape}\")\n    print(f\"  Min: {imgs.min().item():.4f}\")\n    print(f\"  Max: {imgs.max().item():.4f}\")\n    print(f\"  Mean: {imgs.mean().item():.4f}\")\n    print(f\"  Std: {imgs.std().item():.4f}\")\n    \n    print(f\"\\nTarget Parameters:\")\n    print(f\"  Shape: {targets.shape}\")\n    print(f\"  Min: {targets.min().item():.4f}\")\n    print(f\"  Max: {targets.max().item():.4f}\")\n    print(f\"  Mean: {targets.mean().item():.4f}\")\n    print(f\"  Std: {targets.std().item():.4f}\")\n    \n    # Check individual components\n    shape_params = targets[:, :199]\n    exp_params = targets[:, 199:228]\n    pose_params = targets[:, 228:235]\n    \n    print(f\"\\n  Shape params - Mean: {shape_params.mean().item():.4f}, Std: {shape_params.std().item():.4f}\")\n    print(f\"  Exp params   - Mean: {exp_params.mean().item():.4f}, Std: {exp_params.std().item():.4f}\")\n    print(f\"  Pose params  - Mean: {pose_params.mean().item():.4f}, Std: {pose_params.std().item():.4f}\")\n    \n    # WARNING: If any std is > 100, you might need to normalize targets!\n    if shape_params.std().item() > 100:\n        print(\"\\n⚠️  WARNING: Shape parameters have very large values!\")\n        print(\"   Consider normalizing target parameters.\")\n    \n    print(\"=\"*60 + \"\\n\")\n\ncheck_data_statistics(loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Track best model\nbest_loss = float('inf')\nbest_epoch = 0\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n{'='*60}\")\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n    print('='*60)\n        \n    t0 = time.time()\n    epoch_losses = train_epoch(model, loader, optimizer, device, epoch+1)\n    t1 = time.time()\n        \n    # Print epoch summary\n    print(f\"\\nEpoch {epoch+1} Summary:\")\n    print(f\"  Total Loss:      {epoch_losses['total']:.6f}\")\n    print(f\"  Shape Loss:      {epoch_losses['shape']:.6f}\")\n    print(f\"  Expression Loss: {epoch_losses['exp']:.6f}\")\n    print(f\"  Pose Loss:       {epoch_losses['pose']:.6f}\")\n    print(f\"  Time: {t1-t0:.1f}s\")\n        \n    # Save checkpoint if this is the best model so far\n    current_loss = epoch_losses['total']\n    if current_loss < best_loss:\n        best_loss = current_loss\n        best_epoch = epoch + 1\n            \n        torch.save({\n            \"epoch\": epoch+1,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"loss\": current_loss,\n            \"loss_details\": epoch_losses,\n        }, CHECKPOINT_PATH)\n            \n        print(f\"  ✓ NEW BEST MODEL! Loss: {current_loss:.6f} → Saved to {CHECKPOINT_PATH}\")\n    else:\n        print(f\"  Best loss so far: {best_loss:.6f} (Epoch {best_epoch})\")\n    \nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ Training Complete!\")\nprint(f\"✓ Best model from Epoch {best_epoch} with loss {best_loss:.6f}\")\nprint(f\"✓ Saved at: {CHECKPOINT_PATH}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T22:37:06.073Z"}},"outputs":[],"execution_count":null}]}