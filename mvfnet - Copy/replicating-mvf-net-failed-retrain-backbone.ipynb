{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10977544,"sourceType":"datasetVersion","datasetId":6831064},{"sourceId":14066924,"sourceType":"datasetVersion","datasetId":8896393}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, math, csv, time, cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy.io as sio\nfrom collections import OrderedDict\nimport gc\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CONFIGURATION\n# ============================================================================\nSELECTED_FOLDERS = [\"IBUG\", \"IBUG_Flip\"]\nDATA_ROOT = \"/kaggle/input/300w-lp/300W_LP\"\nTRIPLETS_CSV = \"/kaggle/input/multi-view-input/triplets_shorter_IBUG.csv\"\nWEIGHTS_PATH = \"/kaggle/input/multi-view-input/resnet50-11ad3fa6.pth\"\nBATCH_SIZE = 32  # âœ… Increased batch size for better GPU utilization\nNUM_EPOCHS = 2\nCHECKPOINT_PATH = \"resnet50_mvfnet.pth\"\n\nMODEL_SHAPE = \"/kaggle/input/multi-view-input/Model_Shape.mat\"\nMODEL_EXP = \"/kaggle/input/multi-view-input/Model_Expression.mat\"\nDATA_FROM_AUTHOR = \"/kaggle/input/multi-view-input/sigma_exp.mat\"\n\n# Staged training schedule\nTRAINING_SCHEDULE = {\n    # 'stage1': {'epochs': 1, 'lr': 1e-3, 'shape': 1.0, 'exp': 1.0, 'pose': 0.0, 'landmark': 0.0},\n    'stage1': {'epochs': 1, 'lr': 5e-4, 'shape': 1.0, 'exp': 1.0, 'pose': 1.0, 'landmark': 0.0},\n    'stage2': {'epochs': 1, 'lr': 1e-4, 'shape': 1.0, 'exp': 1.0, 'pose': 1.0, 'landmark': 0.01}\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LOAD 3DMM MODELS - MOVE TO GPU IMMEDIATELY\n# ============================================================================\nmodel_shape = sio.loadmat(MODEL_SHAPE)\nmodel_exp = sio.loadmat(MODEL_EXP)\ndata = sio.loadmat(DATA_FROM_AUTHOR)\n\nkpt_index = model_shape[\"keypoints\"].flatten().astype(np.int32) - 1\nshape_std = model_shape[\"sigma\"].flatten() # shape std\nexp_std = model_exp[\"sigma_exp\"].flatten() # exp std cá»§a Model_Exp.mat\n\npose_mean = np.array([0, 0, 0, 112, 112, 0, 0]).astype(np.float32)\npose_std = np.array([\n    math.pi/2.0, math.pi/2.0, math.pi/2.0,\n    56, 56, 1,\n    224.0 / (2 * 180000.0)\n]).astype(np.float32)\n\n# âœ… Convert to tensors and move to GPU ONCE\nw_shape_t = torch.from_numpy(model_shape['w']).float().to(device) #Aid\nw_exp_t = torch.from_numpy(model_exp['w_exp']).float().to(device) #Aexp\nmu_shape_t = torch.from_numpy(model_shape['mu_shape']).float().to(device) #Smu\nsigma_shape_t = torch.from_numpy(model_shape['sigma']).float().to(device) #shape std\nexp_std_t = torch.from_numpy(model_exp[\"sigma_exp\"]).float().to(device) # exp std cá»§a Model_Exp.mat\nsigma_exp_t = torch.from_numpy(data[\"sigma_exp\"]).float().to(device) # ?? exp std cá»§a sigma_exp.mat\nkpt_idx_t = torch.from_numpy(kpt_index).long().to(device)\npose_mean_t = torch.from_numpy(pose_mean).float().to(device)\npose_std_t = torch.from_numpy(pose_std).float().to(device)\n\nprint(\"3DMM models loaded and moved to GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_300wlp_to_bfm_pose(pose_300wlp, origin_w, origin_h, target_size=224):\n    \"\"\"\n    Convert 300W-LP Pose_Para to BFM coordinate system\n    \n    Args:\n        pose_300wlp: (7,) array [pitch, yaw, roll, tx, ty, tz, scale]\n        pitch, yaw, roll in radians in both system\n        tx, ty are in original image coordinates\n        origin_w, origin_h: width, height of original image\n        target_size: BFM input size (default 224)\n    \n    Returns:\n        pose_bfm: (7,) array in BFM coordinate system\n    \"\"\"\n    orig_w = origin_w\n    orig_h = origin_h\n    \n    pitch, yaw, roll = pose_300wlp[0:3]\n    tx_orig, ty_orig = pose_300wlp[3:5]\n    tz_orig = pose_300wlp[5]\n    scale_orig = pose_300wlp[6]\n\n    # Convert rotations to radians\n    # pitch = np.deg2rad(pitch)\n    # yaw   = np.deg2rad(yaw)\n    # roll  = np.deg2rad(roll)\n    \n    scale_factor_x = target_size / orig_w\n    scale_factor_y = target_size / orig_h\n    \n    tx_bfm = tx_orig * scale_factor_x\n    ty_bfm = ty_orig * scale_factor_y\n    scale_bfm = scale_orig \n    tz_bfm = 0.0 # weak-perspective\n    \n    pose_bfm = np.array([\n        pitch, yaw, roll,\n        tx_bfm, ty_bfm, tz_bfm,\n        scale_bfm\n    ], dtype=np.float32)\n    \n    return pose_bfm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LANDMARK LOADING UTILITIES\n# ============================================================================\ndef load_landmarks_from_folder(data_root, folder, mat_filename, origin_w, origin_h, target_size=224):\n    \"\"\"\n    Load landmarks from the landmarks folder\n    \n    Path structure:\n    /kaggle/input/300w-lp/300W_LP/landmarks/LFPW/LFPW_image_test_0001_0_pts.mat\n    \"\"\"\n    # Convert mat filename to pts filename\n    # LFPW_image_test_0001_0.mat -> LFPW_image_test_0001_0_pts.mat\n    pts_filename = mat_filename.replace('.mat', '_pts.mat')\n    \n    landmark_path = os.path.join(data_root, \"landmarks\", folder, pts_filename)\n    \n    if os.path.exists(landmark_path):\n        try:\n            lmk_data = sio.loadmat(landmark_path)\n            pts_2d = lmk_data[\"pts_2d\"].astype(np.float32)  # (68, 2)\n            \n            # Scale to target size\n            scale_x = target_size / origin_w\n            scale_y = target_size / origin_h\n            pts_2d[:, 0] *= scale_x\n            pts_2d[:, 1] *= scale_y\n            \n            return pts_2d, True\n        except Exception as e:\n            print(f\"Warning: Failed to load landmarks from {landmark_path}: {e}\")\n    \n    return None, False\n\ndef load_landmarks_from_mat(mat_path, origin_w, origin_h, target_size=224):\n    \"\"\"\n    Fallback: Load pt2d from the image's .mat file\n    \"\"\"\n    try:\n        mat_data = sio.loadmat(mat_path)\n        pt2d = mat_data[\"pt2d\"].astype(np.float32).T  # (68, 2)\n        \n        # Scale to target size\n        scale_x = target_size / origin_w\n        scale_y = target_size / origin_h\n        pt2d[:, 0] *= scale_x\n        pt2d[:, 1] *= scale_y\n        \n        return pt2d, True\n    except Exception as e:\n        print(f\"Warning: Failed to load pt2d from {mat_path}: {e}\")\n        return None, False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# DATASET\n# ============================================================================\ndef get_folder_from_filename(filename):\n    for folder in SELECTED_FOLDERS:\n        if filename.startswith(folder):\n            return folder\n    raise ValueError(f\"Cannot find folder for {filename}\")\n\nclass Triplet300wLP(Dataset):\n    def __init__(self, triplets_csv, dataset_path, transform=None, target_size=224, use_landmarks_folder=True):\n        self.data = pd.read_csv(triplets_csv)\n        self.dataset_path = dataset_path\n        self.transform = transform\n        self.target_size = target_size\n        self.use_landmarks_folder = use_landmarks_folder\n        \n        self.shape_std = shape_std\n        self.exp_std = exp_std\n        self.pose_mean = pose_mean  \n        self.pose_std = pose_std\n\n        # Check landmarks folder\n        self.landmarks_path = os.path.join(dataset_path, \"landmarks\")\n        self.has_landmarks_folder = os.path.exists(self.landmarks_path)\n        \n        if self.use_landmarks_folder and self.has_landmarks_folder:\n            print(f\"âœ“ Will use landmarks from: {self.landmarks_path}\")\n        else:\n            print(f\"âœ“ Will use pt2d from .mat files\")\n        \n        print(f\"Dataset initialized with {len(self.data)} triplets\")\n        \n        self.folder_map = {}\n        for folder in SELECTED_FOLDERS:\n            folder_path = os.path.join(dataset_path, folder)\n            if not os.path.exists(folder_path):\n                continue\n            for f in os.listdir(folder_path):\n                if f.endswith(\".jpg\") or f.endswith(\".mat\"):\n                    self.folder_map[f] = folder\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        imgs = []\n        front_label = None\n        front_landmarks = None\n        \n        for view in [\"front\", \"left\", \"right\"]:\n            filename_img = row[f'{view}_img']\n            filename_mat = row[f'{view}_mat']\n            \n            folder = get_folder_from_filename(filename_img)\n            img_path = os.path.join(self.dataset_path, folder, filename_img)\n            mat_path = os.path.join(self.dataset_path, folder, filename_mat)\n            \n            img = Image.open(img_path).convert(\"RGB\")\n            orig_w, orig_h = img.size\n            if self.transform:\n                img = self.transform(img)\n            else:\n                img = transforms.ToTensor()(img)\n            imgs.append(img)\n\n            if view == \"front\":\n                mat_data = sio.loadmat(mat_path)\n                shape_para = mat_data[\"Shape_Para\"].flatten()\n                exp_para = mat_data[\"Exp_Para\"].flatten()\n                pose_para_orig = mat_data[\"Pose_Para\"].flatten()\n\n                pose_para = convert_300wlp_to_bfm_pose(\n                    pose_para_orig, \n                    orig_w, orig_h,\n                    self.target_size\n                )\n                \n                # Normalize\n                shape_para = shape_para / self.shape_std\n                exp_para = exp_para / self.exp_std\n                pose_para = (pose_para - self.pose_mean) / self.pose_std\n                \n                front_label = np.concatenate([shape_para, exp_para, pose_para], axis=0)\n\n                if self.use_landmarks_folder and self.has_landmarks_folder:\n                    landmarks, success = load_landmarks_from_folder(\n                        self.dataset_path, folder, filename_mat, \n                        orig_w, orig_h, self.target_size\n                    )\n                    if not success:\n                        landmarks, success = load_landmarks_from_mat(\n                            mat_path, orig_w, orig_h, self.target_size\n                        )\n                else:\n                    landmarks, success = load_landmarks_from_mat(\n                        mat_path, orig_w, orig_h, self.target_size\n                    )\n                \n                if success:\n                    front_landmarks = landmarks\n                else:\n                    front_landmarks = np.zeros((68, 2), dtype=np.float32)\n                    print(f\"Warning: Could not load landmarks for {filename_mat}\")\n        \n        input_tensor = torch.cat(imgs, dim=0)\n        # input_tensor = torch.stack(imgs, dim=0)\n        label_tensor = torch.from_numpy(front_label).float()\n        landmark_tensor = torch.from_numpy(front_landmarks).float()\n        \n        return input_tensor, label_tensor, landmark_tensor\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = Triplet300wLP(TRIPLETS_CSV, DATA_ROOT, transform=transform)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, \n                   num_workers=4, pin_memory=True, prefetch_factor=2)\nprint(f\"Dataset created: {len(dataset)} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TRAIN/TEST SPLIT (80/20)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, \n    [train_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                          num_workers=4, pin_memory=True, prefetch_factor=2)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                         num_workers=4, pin_memory=True, prefetch_factor=2)\n\nprint(f\"Dataset created: {len(dataset)} total samples\")\nprint(f\"  Train: {len(train_dataset)} samples\")\nprint(f\"  Test:  {len(test_dataset)} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MODEL - OPTIMIZED FOR SPEED\n# ============================================================================\nclass ResNetEncoder(nn.Module):\n    def __init__(self, feat_dim=512, num_shape=199, num_exp=29, num_pose=7, weights_path=None):\n        super().__init__()\n\n        self.num_shape = num_shape\n        self.num_exp = num_exp\n        self.num_pose = num_pose\n        \n        # Load ResNet50\n        if weights_path is not None:\n            base_model = models.resnet50(weights=None)\n            state_dict = torch.load(weights_path)\n            base_model.load_state_dict(state_dict)\n        else:\n            base_model = models.resnet50(weights=None)\n        \n        self.backbone = nn.Sequential(*list(base_model.children())[:-2])\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.feat_dim = 2048\n\n        # Learnable fusion weights\n        self.w = nn.Parameter(torch.ones(3, self.feat_dim) / 3.0)\n        \n        self.fc_shape = nn.Sequential(\n            nn.Linear(self.feat_dim * 3, 512),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_shape)\n        )\n        \n        self.fc_exp = nn.Sequential(\n            nn.Linear(self.feat_dim * 3, 512),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_exp)\n        )\n        \n        self.fc_pose = nn.Sequential(\n            nn.Linear(self.feat_dim, 256),\n            nn.ReLU(True),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_pose)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in [self.fc_shape, self.fc_exp]:\n            for layer in m.modules():\n                if isinstance(layer, nn.Linear):\n                    nn.init.xavier_normal_(layer.weight)\n                    if layer.bias is not None:\n                        nn.init.constant_(layer.bias, 0)\n        \n        for layer in self.fc_pose.modules():\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_normal_(layer.weight, gain=0.01)\n                if layer.bias is not None:\n                    nn.init.constant_(layer.bias, 0)\n    \n    def forward(self, x):\n        \"\"\"\n        âœ… OPTIMIZED: Process all 3 views in a single batch pass\n        \"\"\"\n        B = x.size(0)\n        \n        # âœ… Stack all 3 views into one big batch: (B*3, 3, H, W)\n        front = x[:, 0:3, :, :]\n        left = x[:, 3:6, :, :]\n        right = x[:, 6:9, :, :]\n        all_views = torch.cat([front, left, right], dim=0)  # (B*3, 3, 224, 224)\n        \n        # âœ… ONE backbone call for all views!\n        all_feats = self.backbone(all_views)  # (B*3, 2048, 7, 7)\n        all_feats = self.avgpool(all_feats).view(B*3, -1)  # (B*3, 2048)\n        \n        # Split back into 3 views\n        feat_a = all_feats[:B]\n        feat_b = all_feats[B:2*B]\n        feat_c = all_feats[2*B:]    # (B, 2048)\n\n        # Weighted fusion\n        feat_a_w = feat_a * self.w[0]\n        feat_b_w = feat_b * self.w[1]\n        feat_c_w = feat_c * self.w[2]\n        feat_fused = torch.cat([feat_a_w, feat_b_w, feat_c_w], dim=1)\n        \n        # Predict shape & expression\n        # shape_params = self.fc_shape(feat_fused)\n        shape_params = torch.tanh(self.fc_shape(feat_fused)) * 3.0  # Constrained to [-3, +3]\n        exp_params = self.fc_exp(feat_fused)\n        \n        # Predict pose (use original features)\n        pose_a = self.fc_pose(feat_a)\n        pose_b = self.fc_pose(feat_b)\n        pose_c = self.fc_pose(feat_c)\n        \n        # Constrain pose\n        # pose_a = torch.tanh(pose_a) * 3.0\n        # pose_b = torch.tanh(pose_b) * 2.0\n        # pose_c = torch.tanh(pose_c) * 2.0\n        \n        output = torch.cat([shape_params, exp_params, pose_a], dim=1)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LANDMARK PROJECTION - OPTIMIZED\n# ============================================================================\ndef angle_to_rotation_batch(angles):\n    \"\"\"angles: (B, 3) = [pitch (x), yaw (y), roll (z)], return: (B, 3, 3)\"\"\"\n    B = angles.shape[0]\n    device = angles.device\n    \n    # 1. Apply empirical sign flip for 300W-LP compatibility\n    # Keep Pitch and Roll as they are, flip Yaw\n    phi = -angles[:, 0]  # Pitch (X)\n    gamma = -angles[:, 1] # Yaw (Y) - CRITICAL FIX\n    theta = angles[:, 2] # Roll (Z)\n\n    cp, sp = torch.cos(phi), torch.sin(phi)\n    cg, sg = torch.cos(gamma), torch.sin(gamma)\n    ct, st = torch.cos(theta), torch.sin(theta)\n\n    # Batched R_x matrix (Rotation around X - Pitch)\n    R_x = torch.zeros(B, 3, 3, device=device)\n    R_x[:, 0, 0] = 1.0\n    R_x[:, 1, 1] = cp\n    R_x[:, 1, 2] = sp\n    R_x[:, 2, 1] = -sp\n    R_x[:, 2, 2] = cp\n\n    # Batched R_y matrix (Rotation around Y - Yaw)\n    R_y = torch.zeros(B, 3, 3, device=device)\n    R_y[:, 1, 1] = 1.0\n    R_y[:, 0, 0] = cg\n    R_y[:, 0, 2] = -sg\n    R_y[:, 2, 0] = sg\n    R_y[:, 2, 2] = cg\n\n    # Batched R_z matrix (Rotation around Z - Roll)\n    R_z = torch.zeros(B, 3, 3, device=device)\n    R_z[:, 2, 2] = 1.0\n    R_z[:, 0, 0] = ct\n    R_z[:, 0, 1] = st\n    R_z[:, 1, 0] = -st\n    R_z[:, 1, 1] = ct\n\n    # Composition: R = R_x @ R_y @ R_z (sticking to the author's extrinsic order)\n    R = torch.bmm(R_x, torch.bmm(R_y, R_z))\n    \n    return R\n\ndef decode_params(params):\n    \"\"\"\n    Decode parameters: (B, 235) -> 3D shape\n    params: (B, 235) where:\n      - [0:199] = shape parameters\n      - [199:228] = expression parameters  \n      - [228:235] = pose parameters\n    return: shape_3d, R, tx, ty, s\n    \"\"\"\n    B = params.shape[0]\n    \n    shape_norm = params[:, :199]      # (B, 199)\n    exp_norm   = params[:, 199:228]   # (B, 29)\n    pose_norm  = params[:, 228:235]   # (B, 7)\n\n    # DE-NORMALIZE\n    sigma_shape_expanded = sigma_shape_t.squeeze()[None, :].expand(B, -1)  # (B, 199)\n    sigma_exp_expanded = exp_std_t.squeeze()[None, :].expand(B, -1)      # (B, 29)\n    pose_mean_expanded = pose_mean_t[None, :].expand(B, -1)                # (B, 7)\n    pose_std_expanded = pose_std_t[None, :].expand(B, -1)                  # (B, 7)\n    \n    alpha = shape_norm * sigma_shape_expanded\n    # beta  = exp_norm / (1000.0 * sigma_exp_expanded)\n    beta  = exp_norm * sigma_exp_expanded\n    print(alpha.max(), alpha.min(), beta.max(), beta.min())\n\n    pose = pose_norm * pose_std_expanded + pose_mean_expanded\n    \n    tx, ty, tz, s = pose[:,3], pose[:,4], pose[:,5], pose[:,6]\n\n    R = angle_to_rotation_batch(pose[:, :3])\n\n    # 3D FACE SHAPE\n    mu_expanded = mu_shape_t.squeeze()[None, None, :].expand(B, -1, -1)  # (B, 1, N*3)\n    s_comp = torch.matmul(alpha, w_shape_t.transpose(0,1))               # (B, N*3)\n    e_comp = torch.matmul(beta,  w_exp_t.transpose(0,1))                 # (B, N*3)\n    e_comp = e_comp / 1000.0\n    \n    shape_3d = mu_expanded.squeeze(1) + s_comp + e_comp  # (B, N*3)\n    shape_3d = shape_3d.view(B, -1, 3)  # (B, N, 3)\n    # thá»­ rescale láº¡i\n    # shape_3d = shape_3d / 1000.0\n\n    print(\"shape_3d max/min:\", shape_3d.max(), shape_3d.min())\n    print(\"R nan?\", torch.isnan(R).any())\n    print(\"s range:\", s.min(), s.max())\n    print(\"tx range:\", tx.min(), tx.max())\n    print(\"ty range:\", ty.min(), ty.max())\n\n    return shape_3d, R, tx, ty, s\n\n\ndef project_landmarks_batched(params):\n    \"\"\"Project landmarks from 3D face model\"\"\"\n    try:\n        shape_3d, R, tx, ty, s = decode_params(params[:, :235])\n\n        B = shape_3d.shape[0]\n        \n        # Extract landmark indices\n        kpts3d = shape_3d[:, kpt_idx_t, :]  # (B, 68, 3)\n        R2 = R[:, :2, :]  # (B, 2, 3)\n\n        # Project: 2D = s * (R[0:2] @ xyz) + t\n        proj = torch.bmm(kpts3d, R2.transpose(1,2))  # (B, 68, 2)\n        proj = proj * s[:,None,None]  # Scale\n        proj[:,:,0] += tx[:,None]  # Translate X\n        proj[:,:,1] -= ty[:,None]  # Translate Y\n\n        # Flip Y axis\n        proj[:,:,1] = 224 - proj[:,:,1]\n\n        return proj\n    except Exception as e:\n        raise RuntimeError(f\"Landmark projection failed: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# LOSS FUNCTION\n# ============================================================================\ndef compute_loss(pred, target, landmarks_gt, lambda_dict):\n    \"\"\"Compute loss with fixed landmark projection\"\"\"\n    l_s = F.mse_loss(pred[:,:199], target[:,:199])\n    l_e = F.mse_loss(pred[:,199:228], target[:,199:228])\n    l_p = F.mse_loss(pred[:,228:235], target[:,228:235])\n\n    total = (\n        lambda_dict['shape'] * l_s +\n        lambda_dict['exp']   * l_e +\n        lambda_dict['pose']  * l_p\n    )\n\n    l_lmk = torch.tensor(0.0, device=pred.device)\n    \n    if lambda_dict['landmark'] > 0:\n        try:\n            lmk_pred = project_landmarks_batched(pred)  # (B, 68, 2)\n            print(lmk_pred.max(), lmk_pred.min(), torch.isnan(lmk_pred).any())\n            \n            # Ensure shapes match\n            assert lmk_pred.shape == landmarks_gt.shape, \\\n                f\"Shape mismatch: pred {lmk_pred.shape} vs gt {landmarks_gt.shape}\"\n            \n            # Compute Euclidean distance\n            diff = lmk_pred - landmarks_gt  # (B, 68, 2)\n            dist = torch.sqrt((diff ** 2).sum(dim=2) + 1e-8)  # (B, 68)\n            l_lmk = dist.mean()\n            \n            total += lambda_dict['landmark'] * l_lmk\n        except Exception as e:\n            print(f\"Landmark loss error: {str(e)}\")\n            l_lmk = torch.tensor(0.0, device=pred.device)\n\n    return total, {\n        \"total\": total.item(),\n        \"shape\": l_s.item(),\n        \"exp\": l_e.item(),\n        \"pose\": l_p.item(),\n        \"landmark\": l_lmk.item() if not isinstance(l_lmk, float) else l_lmk,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport scipy.io as sio\n\n# Load one sample\nsample_mat_path = \"/kaggle/input/300w-lp/300W_LP/IBUG/IBUG_image_019_1_0.mat\"  # replace with your .mat file\npts_2d_mat_path = \"/kaggle/input/300w-lp/300W_LP/landmarks/IBUG/IBUG_image_019_1_0_pts.mat\"  # if using separate landmarks folder\n\nmat_data = sio.loadmat(sample_mat_path)\nshape_para = mat_data[\"Shape_Para\"].flatten()      # (199,)\nexp_para   = mat_data[\"Exp_Para\"].flatten()        # (29,)\npose_para  = mat_data[\"Pose_Para\"].flatten()       # (7,)\npose_para = convert_300wlp_to_bfm_pose(\n                    pose_para, \n                    450, 450,\n                    224\n                )\n\n# Convert to torch and normalize\nshape_tensor = torch.from_numpy(shape_para / shape_std).float().unsqueeze(0).to(device)\nexp_tensor   = torch.from_numpy(exp_para / exp_std).float().unsqueeze(0).to(device)\npose_tensor  = torch.from_numpy((pose_para - pose_mean) / pose_std).float().unsqueeze(0).to(device)\n\n# Concatenate to full param vector\nparams = torch.cat([shape_tensor, exp_tensor, pose_tensor], dim=1)  # (1, 235)\n\n# Project landmarks\nproj_landmarks = project_landmarks_batched(params)  # (1, 68, 2)\n\n# Load ground-truth 2D landmarks\npts_2d = torch.from_numpy(sio.loadmat(pts_2d_mat_path)['pts_2d'].astype(np.float32)).unsqueeze(0).to(device)\npts_2d[:, 0] *= 224/450\npts_2d[:, 1] *= 224/450\n\n# Compare\nprint(\"Projected landmarks shape:\", proj_landmarks.shape)\nprint(\"First 5 projected landmarks:\\n\", proj_landmarks[0, :5])\nprint(\"First 5 ground-truth landmarks:\\n\", pts_2d[0, :5])\n\n# Optional: L2 error\nl2_error = torch.norm(proj_landmarks - pts_2d, dim=2).mean()\nprint(\"Mean L2 error between projected and GT landmarks:\", l2_error.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TRAINING\n# ============================================================================\nmodel = ResNetEncoder(weights_path=WEIGHTS_PATH).to(device)\n\n# âœ… Use mixed precision training for speed\nscaler = torch.amp.GradScaler('cuda')\n\ndef get_stage_config(epoch):\n    if epoch < 1:\n        return TRAINING_SCHEDULE['stage1']\n    #elif epoch < 2:\n        #return TRAINING_SCHEDULE['stage2']\n    else:\n        return TRAINING_SCHEDULE['stage2']\n\ndef train_epoch(model, dataloader, optimizer, lambda_dict, use_amp=True):\n    model.train()\n    \n    total_samples = 0\n    epoch_losses = {\n        \"total\": 0.0,\n        \"shape\": 0.0,\n        \"exp\": 0.0,\n        \"pose\": 0.0,\n        \"landmark\": 0.0\n    }\n    \n    for batch_idx, (imgs, targets, landmarks_gt) in enumerate(dataloader):\n        imgs = imgs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        landmarks_gt = landmarks_gt.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        \n        # âœ… Mixed precision training\n        if use_amp:\n            with torch.amp.autocast('cuda'):\n                outputs = model(imgs)\n                loss, loss_dict = compute_loss(outputs, targets, landmarks_gt, lambda_dict)\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"WARNING: NaN/Inf loss at batch {batch_idx}, skipping\")\n                continue\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(imgs)\n            loss, loss_dict = compute_loss(outputs, targets, landmarks_gt, lambda_dict)\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"WARNING: NaN/Inf loss at batch {batch_idx}, skipping\")\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        \n        batch_size = imgs.size(0)\n        total_samples += batch_size\n        for key in epoch_losses:\n            epoch_losses[key] += loss_dict[key] * batch_size\n        \n        if (batch_idx + 1) % 100 == 0:\n            print(f\"  Batch {batch_idx+1}/{len(dataloader)} | \"\n                  f\"Loss: {loss_dict['total']:.4f} \"\n                  f\"(S:{loss_dict['shape']:.4f} \"\n                  f\"E:{loss_dict['exp']:.4f} \"\n                  f\"P:{loss_dict['pose']:.4f} \"\n                  f\"L:{loss_dict['landmark']:.2f})\")\n    \n    for key in epoch_losses:\n        epoch_losses[key] /= max(total_samples, 1)\n    \n    return epoch_losses\n\ndef eval_epoch(model, dataloader, lambda_dict):\n    \"\"\"Evaluate on test set\"\"\"\n    model.eval()\n    \n    total_samples = 0\n    epoch_losses = {\n        \"total\": 0.0,\n        \"shape\": 0.0,\n        \"exp\": 0.0,\n        \"pose\": 0.0,\n        \"landmark\": 0.0\n    }\n    \n    with torch.no_grad():\n        for imgs, targets, landmarks_gt in dataloader:\n            imgs = imgs.to(device, non_blocking=True)\n            targets = targets.to(device, non_blocking=True)\n            landmarks_gt = landmarks_gt.to(device, non_blocking=True)\n            \n            outputs = model(imgs)\n            loss, loss_dict = compute_loss(outputs, targets, landmarks_gt, lambda_dict)\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            batch_size = imgs.size(0)\n            total_samples += batch_size\n            for key in epoch_losses:\n                epoch_losses[key] += loss_dict[key] * batch_size\n    \n    for key in epoch_losses:\n        epoch_losses[key] /= max(total_samples, 1)\n    \n    return epoch_losses\n\n# Training loop\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING STAGED TRAINING\")\nprint(\"=\"*60)\n\nbest_loss = float('inf')\nbest_epoch = 0\noptimizer = None\ncurrent_lr = None\n\nfor epoch in range(NUM_EPOCHS):\n    stage_config = get_stage_config(epoch)\n    \n    if current_lr != stage_config['lr']:\n        current_lr = stage_config['lr']\n        optimizer = torch.optim.Adam(model.parameters(), lr=current_lr)\n        print(f\"\\nðŸ”„ Learning rate changed to {current_lr}\")\n    \n    lambda_dict = {\n        'shape': stage_config['shape'],\n        'exp': stage_config['exp'],\n        'pose': stage_config['pose'],\n        'landmark': stage_config['landmark']\n    }\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n    if epoch < 1:\n        print(\"ðŸ”´ STAGE 1: Learning Shape & Expression only\")\n    # elif epoch < 2:\n        # print(\"ðŸŸ¡ STAGE 2: Adding Pose\")\n    else:\n        print(\"ðŸŸ¢ STAGE 3: Full training with Landmarks\")\n    print(f\"Lambda: S={lambda_dict['shape']:.3f} E={lambda_dict['exp']:.3f} \"\n          f\"P={lambda_dict['pose']:.4f} L={lambda_dict['landmark']:.4f}\")\n    print('='*60)\n        \n    t0 = time.time()\n    epoch_losses = train_epoch(model, train_loader, optimizer, lambda_dict, use_amp=True)\n    test_losses = eval_epoch(model, test_loader, lambda_dict)\n    t1 = time.time()\n        \n    print(f\"\\nEpoch {epoch+1} Summary:\")\n    print(f\"  TRAIN - Total: {epoch_losses['total']:.6f}, Shape: {epoch_losses['shape']:.6f}, Exp: {epoch_losses['exp']:.6f}, Pose: {epoch_losses['pose']:.6f}, Landmark: {epoch_losses['landmark']:.4f}\")\n    print(f\"  TEST  - Total: {test_losses['total']:.6f}, Shape: {test_losses['shape']:.6f}, Exp: {test_losses['exp']:.6f}, Pose: {test_losses['pose']:.6f}, Landmark: {test_losses['landmark']:.4f}\")\n    print(f\"  Time: {t1-t0:.1f}s\")\n        \n    current_loss = test_losses['total']\n    if current_loss < best_loss:\n        best_loss = current_loss\n        best_epoch = epoch + 1\n            \n        torch.save({\n            \"epoch\": epoch+1,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"train_loss\": epoch_losses['total'],\n            \"test_loss\": test_losses['total'],\n            \"train_losses\": epoch_losses,\n            \"test_losses\": test_losses,\n        }, CHECKPOINT_PATH)\n            \n        print(f\"  âœ“ NEW BEST MODEL! Test Loss: {current_loss:.6f}\")\n    else:\n        print(f\"  Best test loss: {best_loss:.6f} (Epoch {best_epoch})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ“ Training Complete!\")\nprint(f\"âœ“ Best model from Epoch {best_epoch} with loss {best_loss:.6f}\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}